---
title: "Evaluating models"
format: gfm
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(paged.print = FALSE)
```

```{r}
#| message: false
library(data.table)
```


# Evaluating classification models

```{r}
spam_d <- fread("repo-clone/Spambase/spamD.tsv", 
                header = T, sep = "\t")
spam_train <- spam_d[rgroup >= 10, ]
spam_test <- spam_d[rgroup < 10, ]
spam_vars <- setdiff(colnames(spam_d), list("rgroup", "spam"))

spam_formula <- as.formula(
  paste('spam == "spam"', paste(spam_vars, collapse = ' + '),
        sep = ' ~ '))
spam_model <- glm(spam_formula, family = binomial(link = 'logit'),
                  data = spam_train)
spam_train$pred <- predict(spam_model, newdata = spam_train,
                           type = 'response')
spam_test$pred <- predict(spam_model, newdata = spam_test,
                          type = "response")
```

```{r}
sample <- spam_test[c(7, 35, 224, 327), c("spam", "pred")]
sample
```

## Confusion matrix

```{r}
confmat_spam <- table(
  truth = spam_test$spam,
  prediction = ifelse(spam_test$pred > 0.5, "spam", "non-spam")
)
confmat_spam
```

## Precision, recall, F1, specificity

```{r}
(precision <- confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[1,2]))
(recall <- confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[2,1]))
(F1 <- 2 * precision * recall / (precision + recall))
(specificity <- confmat_spam[1,1] / (confmat_spam[1,1] + confmat_spam[1,2]))
```

How the volume of spam affects filter performance

```{r}
set.seed(234641)
N <- nrow(spam_test)
pull_out_idx <- sample.int(N, 100, replace = F)
removed <- spam_test[pull_out_idx]

get_performance <- function(test) {
  proportion <- mean(test$spam == "spam")
  confmat_spam <- table(truth = test$spam,
                        prediction = ifelse(test$pred > 0.5,
                                            "spam", "non-spam"))
  precision <- confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[1,2])
  recall <- confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[2,1])
  list(spam_proportion = proportion,
       confmat_spam = confmat_spam,
       precision = precision, recall = recall)
}
s_test <- spam_test[-pull_out_idx, ]
get_performance(s_test)
```

Add extra spam

```{r}
get_performance(rbind(s_test, subset(removed, spam == "spam")))
```

Add extra non-spam

```{r}
get_performance(rbind(s_test, subset(removed, spam == "non-spam")))
```

## Evaluating scoring models

Predict temperature from cricket chirp rate

```{r}
crickets <- fread("repo-clone/cricketchirps/crickets.csv")
head(crickets)
```

```{r}
cricket_model <- lm(temperatureF ~ chirp_rate, data = crickets)
crickets$temp_pred <- predict(cricket_model, newdata = crickets)
```

```{r pracr-6-model-eval-1}
library(ggplot2)
ggplot(crickets, aes(x = chirp_rate, y = temperatureF)) +
  geom_point(color = "red") +
  geom_smooth(method = "lm", se = F) +
  geom_segment(aes(x = chirp_rate, xend = chirp_rate, 
                   y = temp_pred, yend = temperatureF),
    color = "black", linetype = "dashed"
  ) +
  theme_bw() +
  xlab("Chirp rate") +
  ylab("Temperature (F)") +
  ggtitle("Actual and predicted temperature (F) as a function of cricket chirp rate")
```

```{r}
cricket_model
```

```{r}
summary(cricket_model)
```

RMSE

```{r}
sqrt(mean((crickets$temp_pred - crickets$temperatureF)^2))
```

R-squared

```{r}
error_sq <- (crickets$temp_pred - crickets$temperatureF)^2
numerator <- sum(error_sq)
delta_sq <- (mean(crickets$temperatureF) - crickets$temperatureF)^2
denominator <- sum(delta_sq)

(R2 <- 1 - numerator / denominator)
```

## Evaluating probability models

### Double density plot

```{r pracr-6-model-eval-2}
#| message: false
library(WVPlots)
DoubleDensityPlot(spam_test,
                  xvar = "pred", truthVar = "spam",
                  title = "Distributin of scores for spam filter")
```

### ROC and AUC

```{r pracr-6-model-eval-3}
ROCPlot(spam_test,
        xvar = "pred",
        truthVar = "spam", truthTarget = "spam",
        title = "Spam filter test performance")
```

```{r}
sigr::calcAUC(spam_test$pred, spam_test$spam == "spam")
```

### Log likelihood

```{r}
ylogpy <- function(y, py) {
  logpy <- ifelse(py > 0, log(py), 0)
  y * logpy
}

y <- spam_test$spam == "spam"
sum(ylogpy(y, spam_test$pred) +
      ylogpy(1 - y, 1 - spam_test$pred))
```

> null model's likelihood

```{r}
(p_null <- mean(spam_train$spam == "spam"))
sum(ylogpy(y, p_null) + ylogpy(1 - y, 1 - p_null))
```

Log likelihood of the model is better.

## Deviance

`-2*(logLikelihood - S)` where is is the log likelihood of the saturated model. In most cases, the model is perfect so `S=0`.

The ratio between the null deviance and the model deviance is used to calculate a _pseudo R-squared_.

```{r}
#| message: false
library(sigr)
(deviance <- calcDeviance(spam_test$pred,
                          spam_test$spam == "spam"))
(null_deviance <- calcDeviance(p_null, spam_test$spam == "spam"))
(pseudo_R2 <- 1 - deviance / null_deviance)
```

# Local interpretable model-agnostic explanations (LIME)

LIME produces an “explanation” of a model’s prediction on a specific datum. That is, LIME tries to determine which features of that datum contributed the most to the model’s decision about it.

## Example

Suppose you have a dataset of petal and sepal measurements for three varieties of iris. The object is to predict whether a given iris is a setosa based on its petal and sepal dimensions.

```{r}
iris <- iris
iris$class <- as.numeric(iris$Species == "setosa")
```

```{r}
set.seed(2345)
in_train <- runif(nrow(iris)) < 0.75
train <- iris[in_train, ]
test <- iris[!in_train, ]
head(train)
```

```{r}
source("repo-clone/LIME_iris/lime_iris_example.R")

input <- as.matrix(train[, 1:4])
model <- fit_iris_example(input, train$class)
```

```{r}
predictions <- predict(model, newdata = as.matrix(test[, 1:4]))
te_frame <- data.frame(is_setosa = ifelse(test$class == 1,
                                         "setosa", "not setosa"),
                       pred = ifelse(predictions > 0.5,
                                     "setosa", "not setosa"))
with(te_frame, table(truth = is_setosa, pred = pred))
```

The classification is perfect. Use the `lime` package to explain an instance.

```{r}
library(lime)
explainer <- lime(train[,1:4],
                  model = model,
                  bin_continuous = T,
                  n_bins = 10)
```

```{r}
(example <- test[5, 1:4, drop = F])
test$class[5]
round(predict(model, newdata = as.matrix(example)))
```

```{r pracr-6-model-eval-4}
explanation <- lime::explain(example,
                             explainer,
                             n_label = 1,
                             n_features = 4)
plot_features(explanation)
```

```{r}
(example <- test[c(13, 24), 1:4])
test$class[c(13, 24)]
round(predict(model, newdata = as.matrix(example)))
```

```{r pracr-6-model-eval-5}
explanation <- explain(
  example, explainer, n_labels = 1, n_features = 4, kernel_width = 0.5
)
plot_features(explanation)
```

## Text classification

For this example, you will classify movie reviews from the Internet Movie Database (IMDB). The task is to identify positive reviews.

Each RDS object is a list with two elements: a character vector representing 25,000 reviews, and a vector of numeric labels where 1 means a positive review and 0 a negative review. 13 You will again fit an xgboost model to classify the reviews.

You might wonder how LIME jitters a text datum. It does so by randomly removing words from the document, and then converting the resulting new text into the appropriate representation for the model. If removing a word tends to change the classification of a document, then that word is probably important to the model.

```{r}
library(zeallot)
c(texts, labels) %<-% readRDS("repo-clone/IMDB/IMDBtrain.RDS")
```

A positive review

```{r}
list(text = texts[1], label = labels[1])
```

A negative review

```{r}
list(text = texts[12], label = labels[12])
```

## Representing documents for modeling

Avoid words that are too common or too rare. Too common will be words that show up in over half the documents, too rare will be words that show up in fewer than 0.1% of the documents. Build a vocabulary of 10,000 words.

## Training the text classifier

```{r}
source("repo-clone/IMDB/lime_imdb_example.R")

vocab <- create_pruned_vocabulary(texts)
dtm_train <- make_matrix(texts, vocab)
model <- fit_imdb_model(dtm_train, labels)
```

Load test data and evaluate the model

```{r}
c(test_txt, test_labels) %<-% readRDS("repo-clone/IMDB/IMDBtest.RDS")
dtm_test <- make_matrix(test_txt, vocab)
predicted <- predict(model, newdata = dtm_test)
te_frame <- data.frame(
  true_label = test_labels,
  pred = predicted)
(cmat <- with(te_frame, table(truth = true_label,
                              pred = pred > 0.5)))
```

```{r pracr-6-model-eval-6}
DoubleDensityPlot(te_frame, "pred", "true_label",
                  "Distribution of test prediction scores")
```

## Explaining predictions

```{r}
casename <- "test_19552"
explain_case <- function(casename, model, vocabin) {
  sample_case <- test_txt[casename]
  pred_prob <- predict(model, make_matrix(sample_case, vocabin))
  list(text = sample_case,
       label = test_labels[casename],
       prediction = round(pred_prob))
}
explain_case(casename, model, vocab)
```

```{r pracr-6-model-eval-7}
explainer <- lime(texts, model = model,
                  preprocess = function(x) make_matrix(x, vocab))
explainer_case <- function(casename, explainer, n_labels = 1, n_features = 5) {
  sample_case <- test_txt[casename]
  lime::explain(sample_case,
                 explainer,
                 n_labels = 1,
                 n_features = 5)
}
explanation <- explainer_case(casename, explainer)
plot_features(explanation)
```

This only works from the console.

```{r}
#| eval: false
plot_text_explanations(explanation)
```

```{r}
casenames <- c("test_12034", "test_10294")
explain_case(casenames, model, vocab)
```

```{r pracr-6-model-eval-8}
explanation <- explainer_case(casenames, explainer)
plot_features(explanation)
```

Note the uncertainty for case 2 and the strange choice of key words. 

```{r}
predict(model, newdata = make_matrix(test_txt[casenames][2],
                                     vocab))
```

```{r}
test_txt[casenames][2]
```

The discrepancy between the model's prediction and the explainer's estimate is due to the explainer using a linear approximation of the model rather than the model itself.

The number 8 is likely due to numerical reviews. We can remove numbers from the vocabulary.

```{r}
vocab[is.numeric(vocab$term)]
str(vocab)
```

```{r}
numbers_only <- function(x) !grepl("\\D", x)
vocab_nonums <- vocab[!numbers_only(vocab$term),]
length(vocab_nonums$term)
```

```{r}
dtm_train_nonums <- make_matrix(texts, vocab_nonums)
model_nonums <- fit_imdb_model(dtm_train_nonums, labels)
```

```{r}
casenames <- c("test_12034", "test_10294")
explain_case(casenames, model_nonums, vocab_nonums)
```

```{r}
explainer_nonums <- lime(texts, model = model_nonums,
                         preprocess = function(x) make_matrix(x, vocab_nonums))
```


```{r pracr-6-model-eval-9}
explanation_nonums <- explainer_case(casenames, explainer_nonums)
plot_features(explanation_nonums)
```











